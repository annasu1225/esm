{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395c2027",
   "metadata": {},
   "source": [
    "# Tutorial 6: Protein Structure Embedding with ESM3\n",
    "\n",
    "In this notebook we will see how to embed a batch of structures using ESM 3, as well as explore its different layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e456eef",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e1f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from esm.models.esm3 import ESM3\n",
    "from cookbook.snippets.structure_embed import get_layer_embedding\n",
    "from esm.utils.constants import esm3 as C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda4e7a",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2222ea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Loading ESM3 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e0ed9c6cd74e2cba75f06d2fedc1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vast/palmer/home.mccleary/as4272/protein_design/esm/esm/pretrained.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Loading ESM3 model...\")\n",
    "model = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644f39b",
   "metadata": {},
   "source": [
    "# Getting embeddings from specific layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08f750",
   "metadata": {},
   "source": [
    "We will demo two use cases:\n",
    "1. Getting embeddings for complete protein structure\n",
    "2. Getting embeddings for only backbone structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e94fdf",
   "metadata": {},
   "source": [
    "## 1. Getting embeddings for complete protein structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937a75d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b247b75",
   "metadata": {},
   "source": [
    "## 2. Getting embeddings for only backbone structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "751eee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading protein chain for PDB ID: 1mjc, Chain ID: A\n",
      "Number of residues with all backbone atoms present: 69 out of 69\n",
      "<class 'numpy.ndarray'>\n",
      "Filtered backbone coordinate shape: torch.Size([1, 69, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "pdb_id = \"1mjc\"\n",
    "chain_id = \"A\"\n",
    "layer = 45\n",
    "structure_tokens = get_layer_embedding(pdb_id, chain_id, model, layer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6aa3be",
   "metadata": {},
   "source": [
    "Checking backbone structure token correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b640713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 721, 2431, 1244, 3952, 2165, 3892, 3703, 3967, 2435, 2784, 3203, 3058,\n",
      "          745, 4006, 1412, 3414, 2463,  638, 3304, 1439,  257, 3376, 1397, 1400,\n",
      "         1083, 2871, 3337,  815, 3912, 2971, 1466,  602, 2348, 2605, 1725, 2881,\n",
      "         1921,   96, 1853,  497,  446, 3089, 3232, 3350,  698, 1829,  546,  481,\n",
      "          567, 1451, 1247,  973, 2004, 1877, 1973,  448,  747, 2983, 4073, 2157,\n",
      "         2824, 3968, 2015,   26,  612,  592, 2977,  615, 3234]])\n",
      "number of unique structure tokens: 69\n"
     ]
    }
   ],
   "source": [
    "print(structure_tokens)\n",
    "print(f\"number of unique structure tokens: {structure_tokens.unique().numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f443966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLDDT: 0.8731199502944946\n",
      "pTM: 0.7162373661994934\n"
     ]
    }
   ],
   "source": [
    "from esm.utils.decoding import decode_structure\n",
    "from esm.sdk.api import ESMProtein\n",
    "\n",
    "def add_bos_eos(tokens: torch.Tensor):\n",
    "    if tokens.ndim == 2:\n",
    "        tokens = tokens.squeeze(0)\n",
    "    bos = torch.tensor([C.VQVAE_SPECIAL_TOKENS[\"BOS\"]], device=tokens.device)\n",
    "    eos = torch.tensor([C.VQVAE_SPECIAL_TOKENS[\"EOS\"]], device=tokens.device)\n",
    "    return torch.cat([bos, tokens, eos], dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_from_structure_tokens(structure_tokens, model, device):\n",
    "    structure_tokens_with_special = add_bos_eos(structure_tokens)\n",
    "    \n",
    "    atom37_coords, plddt, ptm = decode_structure(\n",
    "        structure_tokens_with_special,\n",
    "        structure_decoder=model.get_structure_decoder(),\n",
    "        structure_tokenizer=model.tokenizers.structure,\n",
    "        sequence=None\n",
    "    )\n",
    "    return atom37_coords, plddt, ptm\n",
    "\n",
    "recon_coords, plddt, ptm = decode_from_structure_tokens(structure_tokens, model, device)\n",
    "\n",
    "protein = ESMProtein(coordinates=recon_coords.to(device).numpy())\n",
    "\n",
    "print(\"PLDDT:\", plddt.mean().item())\n",
    "print(\"pTM:\", ptm.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
